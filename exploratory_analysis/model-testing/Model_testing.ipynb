{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lyrics(song_link):\n",
    "  response = get(song_link)\n",
    "  html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  lyrics = html_soup.find_all('p')\n",
    "  song = []\n",
    "  for paragraph in lyrics:\n",
    "    paragraph = str(paragraph).replace(\"<br/>\", \"\\s\").replace(\"</p>\", \"\").replace(\"<p>\", \"\").replace(\"!\", \"\")\n",
    "    song.append(paragraph)\n",
    "  del song[-2:]\n",
    "  song = '\\n'.join(song)\n",
    "  return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connect with URL...\")\n",
    "url = 'https://www.nickcave.com/lyrics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get(url)\n",
    "url = 'https://www.nickcave.com/lyrics/'\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "albums = html_soup.find_all('div', class_ = 'lyric-album-list')\n",
    "print(f'Number of available albums: {len(albums)}')\n",
    "\n",
    "\n",
    "file  = io.open(\"songs.txt\", \"w\", encoding=\"utf-8\") \n",
    "\n",
    "print('Parse text...')\n",
    "\n",
    "no_songs = 0\n",
    "\n",
    "for album in albums:\n",
    "  songs = album.find_all('a')\n",
    "  for song in songs:\n",
    "    song_link = song.get('href')\n",
    "    file.write(parse_lyrics(song_link)+'\\s')\n",
    "    no_songs += 1\n",
    "\n",
    "print(f\"No. of downloaded songs: {no_songs}\")\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(f\"'Songs.txt' file succesfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer.\n"
     ]
    }
   ],
   "source": [
    "## Build tokenizer\n",
    "from aitextgen import aitextgen\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "\n",
    "file_name = \"songs.txt\"\n",
    "\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Constructing GPT-2 model from provided config.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    }
   ],
   "source": [
    "## Train model\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=GPT2ConfigCPU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb08748c2093437a897e371a9074fe3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=1450.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.TokenDataset:Encoding 1,450 sets of tokens from songs.txt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Create TokenDatasets, that builds datasets for training, processing them with the appropriate size.\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "WARNING:lightning:No environment variable for node rank defined. Set as 0.\n",
      "/Users/stasianik/opt/anaconda3/envs/insight/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dc03c4ef704f1295cfe0c4cdef8b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=10000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\\s And my girl\\s O\\s And it’s And I’t me\\s I’s a long a morning\\s And the night I’t you’m a my my love\\s And I’t your road\\s I’t got a head\\s I’\n",
      "==========\n",
      "\u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\\s A man in the pain, my little world\\s The bathtub\\s There baby’s But I don’t no long time, I ain’t got a little long\n",
      "The night, my girl and a little eyes\\s When she was a little heart of the man\n",
      "I’t\n",
      "==========\n",
      "\u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " up me\\s You are all the sun, my baby and my hand\\s And I’ll be my baby, my baby?\\s We had a little Lord\\s I comes\\s This is a little little girl she will be my baby?\\s There is a hands\\s I love baby?\\\n",
      "==========\n",
      "\u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " at\\s She’ll get the sea\\s I was all away\n",
      "I’m a tiny little gun\\s And I’ll go\\s I ain’t be again\n",
      " \n",
      "I said, I don’t know I’t no more time\\s She I have been blind\\s\n",
      "==========\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\\s Yeah now\\s Now you come on the stars are\\s Hey, I am what to come\\s I am on\\s When I’ve been a dream\\s A little gun in my baby’m on the sun\\s You are a little girl the name of the hill\\s\n",
      "==========\n",
      "\u001b[1m6,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m6,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " his heart-o\\s The sun it’s The lers the gates of the morning\\s The night of the sun did\\s The sun was Elisa Day\n",
      "Yes that I’ve been a real cool\\s They’m a little girl is the ground\\s That I’m want to\n",
      "==========\n",
      "\u001b[1m7,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m7,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " &amp; the streets &amp; the rain on the sea\\s in the door\\s &amp; I see &amp; do it’ve been on the winded here\\s &amp; the stars\\s &amp; I am be my girl)\\s I’m born to be\n",
      "==========\n",
      "\u001b[1m8,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m8,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\\s We’re all to the door\\s But by the moon\\s And the water-cr yourself\\s And the streets and lonely.\\s And the world is it am the stars will\\s Just how to the world\n",
      "And she was blind\\s And you know she comes down\\s\n",
      "==========\n",
      "\u001b[1m9,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m9,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " out of the street\\s O Mamma O Mamma\n",
      "You are all the bathtub but through the moon\\s The road is your tears\\s And I’ve been out in the sun was a man in a wife\\s And I’ve loved you for love\\s I give me for long\n",
      "Now\n",
      "==========\n",
      "\u001b[1m10,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m10,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Saving trained model pytorch_model.bin to /trained_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "==========\n",
      "\r",
      "es\\s I think I saw a-m the hammer\n",
      "Am gonna the sea”\\s And I felt a-bong?\n",
      "And I’m gonna be a real in your hands\\s Her hair was a children\n",
      "I’m about this child\\s I’ve been a little train\\\n",
      "\r",
      "==========\n"
     ]
    }
   ],
   "source": [
    "## Train!\n",
    "ai.train(data, batch_size=16, num_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Generating 3 texts to ATG_20200604_122536_59718989.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296ff84b9744477a960b35226035f2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ai.generate_to_file(n=3, prompt=\"We drowned\", max_length=100, temperature=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
